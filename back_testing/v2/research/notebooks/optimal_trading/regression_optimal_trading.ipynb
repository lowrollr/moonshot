{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38564bitbf960d00bf8447f58cc83a94e3f130e4",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "/Users/rosscopeland/Desktop/personal/code/vivaldi/back_testing\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir('../../../../')\n",
    "print(os.getcwd())\n",
    "from v2.strategy.indicators.optimal_v2 import Optimal_v2\n",
    "from v2.model import Trading\n",
    "from v2.strategy.indicators.param import Param\n",
    "from v2.strategy.indicators.notebook_utils import fetchIndicators, genDataForAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    my_config = {}\n",
    "    with open('config.config') as config:\n",
    "        for line in config:\n",
    "            args = line.split('=')\n",
    "            my_config[args[0]] = args[1].rstrip().split(',')\n",
    "    return my_config\n",
    "\n",
    "model = Trading(load_config())\n",
    "dataset = model.dfs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal = Optimal_v2(_params=[], _name='optimal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Features</h3>\n",
    "<p>Close</p>\n",
    "<p>RSI</p>\n",
    "<p>MACD</p>\n",
    "<p>Oscillator</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_inds = fetchIndicators([\"rsi\", \"macd\", \"stochastic_oscillator\", \"optimal_v2\"])\n",
    "genDataForAll(dataset, my_inds)\n",
    "dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               time        open        high         low        close  \\\n358      1383503820   207.67198   207.67198   207.67198   207.562051   \n359      1383505860   207.43988   207.43988   206.89000   207.287911   \n360      1383507660   207.59295   207.75000   207.59295   207.518955   \n361      1383507720   207.75000   207.75000   207.75000   207.634478   \n362      1383510600   207.75000   207.75000   207.75000   207.692239   \n...             ...         ...         ...         ...          ...   \n1777671  1593561180  9147.00000  9147.00000  9147.00000  9147.145284   \n1777672  1593561240  9147.00000  9147.00000  9145.70000  9146.422642   \n1777673  1593561360  9144.70000  9144.70000  9133.40000  9139.911321   \n1777674  1593561480  9138.40000  9138.40000  9138.40000  9139.155661   \n1777675  1593561540  9137.10000  9137.10000  9133.90000  9136.527830   \n\n            volume  trades  rsi_diff    rsi_u     rsi_d  ...     ema_slow  \\\n358       0.100000       1  -0.00418  0.00000   0.00418  ...   203.646855   \n359       1.263692       5  -0.65821  0.00000   0.65821  ...   203.702506   \n360       0.100000       2   0.73623  0.73623   0.00000  ...   203.769407   \n361       0.100000       1   0.00000  0.00000   0.00000  ...   203.835202   \n362       0.314611       1   0.00000  0.00000   0.00000  ...   203.899909   \n...            ...     ...       ...      ...       ...  ...          ...   \n1777671   0.002000       1   2.00000  2.00000   0.00000  ...  9145.482921   \n1777672   0.187285       5  -1.30000  0.00000   1.30000  ...  9145.486510   \n1777673  23.300000      36 -12.30000  0.00000  12.30000  ...  9145.286733   \n1777674   0.005033       1   5.00000  5.00000   0.00000  ...  9145.172902   \n1777675   0.933489       5  -4.50000  0.00000   4.50000  ...  9144.986573   \n\n            ema_fast      MACD    signal  macd_diff  stosc_high_price  \\\n358       205.570957  1.924102  2.995043  -1.070941            201.04   \n359       205.618262  1.915756  2.971323  -1.055567            201.04   \n360       205.688155  1.918748  2.948189  -1.029441            201.04   \n361       205.755757  1.920555  2.925604  -1.005049            201.04   \n362       205.821142  1.921233  2.903530  -0.982297            201.04   \n...              ...       ...       ...        ...               ...   \n1777671  9145.263964 -0.218958 -1.056774   0.837816           9127.00   \n1777672  9145.278260 -0.208250 -1.038125   0.829875           9127.00   \n1777673  9144.888809 -0.397924 -1.024055   0.626131           9127.00   \n1777674  9144.676061 -0.496841 -1.012468   0.515626           9127.00   \n1777675  9144.322747 -0.663826 -1.004805   0.340979           9127.00   \n\n         stosc_low_price    stosc_k    stosc_d   optimal  \n358            207.96822   4.275846  23.660815 -0.715267  \n359            207.96822  13.776266  23.698312  0.627125  \n360            207.96822   3.149727  23.696407  0.000000  \n361            207.96822   3.149727  23.694475  0.000000  \n362            207.96822   3.149727  23.696210  0.000000  \n...                  ...        ...        ...       ...  \n1777671       9158.30000  36.102236  38.304047  0.000000  \n1777672       9158.30000  40.255591  38.084277  0.000000  \n1777673       9158.30000  79.552716  38.010028  0.000000  \n1777674       9158.30000  63.578275  37.912694  0.000000  \n1777675       9158.30000  77.955272  37.897886  0.000000  \n\n[1777042 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>trades</th>\n      <th>rsi_diff</th>\n      <th>rsi_u</th>\n      <th>rsi_d</th>\n      <th>...</th>\n      <th>ema_slow</th>\n      <th>ema_fast</th>\n      <th>MACD</th>\n      <th>signal</th>\n      <th>macd_diff</th>\n      <th>stosc_high_price</th>\n      <th>stosc_low_price</th>\n      <th>stosc_k</th>\n      <th>stosc_d</th>\n      <th>optimal</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>358</th>\n      <td>1383503820</td>\n      <td>207.67198</td>\n      <td>207.67198</td>\n      <td>207.67198</td>\n      <td>207.562051</td>\n      <td>0.100000</td>\n      <td>1</td>\n      <td>-0.00418</td>\n      <td>0.00000</td>\n      <td>0.00418</td>\n      <td>...</td>\n      <td>203.646855</td>\n      <td>205.570957</td>\n      <td>1.924102</td>\n      <td>2.995043</td>\n      <td>-1.070941</td>\n      <td>201.04</td>\n      <td>207.96822</td>\n      <td>4.275846</td>\n      <td>23.660815</td>\n      <td>-0.715267</td>\n    </tr>\n    <tr>\n      <th>359</th>\n      <td>1383505860</td>\n      <td>207.43988</td>\n      <td>207.43988</td>\n      <td>206.89000</td>\n      <td>207.287911</td>\n      <td>1.263692</td>\n      <td>5</td>\n      <td>-0.65821</td>\n      <td>0.00000</td>\n      <td>0.65821</td>\n      <td>...</td>\n      <td>203.702506</td>\n      <td>205.618262</td>\n      <td>1.915756</td>\n      <td>2.971323</td>\n      <td>-1.055567</td>\n      <td>201.04</td>\n      <td>207.96822</td>\n      <td>13.776266</td>\n      <td>23.698312</td>\n      <td>0.627125</td>\n    </tr>\n    <tr>\n      <th>360</th>\n      <td>1383507660</td>\n      <td>207.59295</td>\n      <td>207.75000</td>\n      <td>207.59295</td>\n      <td>207.518955</td>\n      <td>0.100000</td>\n      <td>2</td>\n      <td>0.73623</td>\n      <td>0.73623</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>203.769407</td>\n      <td>205.688155</td>\n      <td>1.918748</td>\n      <td>2.948189</td>\n      <td>-1.029441</td>\n      <td>201.04</td>\n      <td>207.96822</td>\n      <td>3.149727</td>\n      <td>23.696407</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>361</th>\n      <td>1383507720</td>\n      <td>207.75000</td>\n      <td>207.75000</td>\n      <td>207.75000</td>\n      <td>207.634478</td>\n      <td>0.100000</td>\n      <td>1</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>203.835202</td>\n      <td>205.755757</td>\n      <td>1.920555</td>\n      <td>2.925604</td>\n      <td>-1.005049</td>\n      <td>201.04</td>\n      <td>207.96822</td>\n      <td>3.149727</td>\n      <td>23.694475</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>362</th>\n      <td>1383510600</td>\n      <td>207.75000</td>\n      <td>207.75000</td>\n      <td>207.75000</td>\n      <td>207.692239</td>\n      <td>0.314611</td>\n      <td>1</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>203.899909</td>\n      <td>205.821142</td>\n      <td>1.921233</td>\n      <td>2.903530</td>\n      <td>-0.982297</td>\n      <td>201.04</td>\n      <td>207.96822</td>\n      <td>3.149727</td>\n      <td>23.696210</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1777671</th>\n      <td>1593561180</td>\n      <td>9147.00000</td>\n      <td>9147.00000</td>\n      <td>9147.00000</td>\n      <td>9147.145284</td>\n      <td>0.002000</td>\n      <td>1</td>\n      <td>2.00000</td>\n      <td>2.00000</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>9145.482921</td>\n      <td>9145.263964</td>\n      <td>-0.218958</td>\n      <td>-1.056774</td>\n      <td>0.837816</td>\n      <td>9127.00</td>\n      <td>9158.30000</td>\n      <td>36.102236</td>\n      <td>38.304047</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1777672</th>\n      <td>1593561240</td>\n      <td>9147.00000</td>\n      <td>9147.00000</td>\n      <td>9145.70000</td>\n      <td>9146.422642</td>\n      <td>0.187285</td>\n      <td>5</td>\n      <td>-1.30000</td>\n      <td>0.00000</td>\n      <td>1.30000</td>\n      <td>...</td>\n      <td>9145.486510</td>\n      <td>9145.278260</td>\n      <td>-0.208250</td>\n      <td>-1.038125</td>\n      <td>0.829875</td>\n      <td>9127.00</td>\n      <td>9158.30000</td>\n      <td>40.255591</td>\n      <td>38.084277</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1777673</th>\n      <td>1593561360</td>\n      <td>9144.70000</td>\n      <td>9144.70000</td>\n      <td>9133.40000</td>\n      <td>9139.911321</td>\n      <td>23.300000</td>\n      <td>36</td>\n      <td>-12.30000</td>\n      <td>0.00000</td>\n      <td>12.30000</td>\n      <td>...</td>\n      <td>9145.286733</td>\n      <td>9144.888809</td>\n      <td>-0.397924</td>\n      <td>-1.024055</td>\n      <td>0.626131</td>\n      <td>9127.00</td>\n      <td>9158.30000</td>\n      <td>79.552716</td>\n      <td>38.010028</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1777674</th>\n      <td>1593561480</td>\n      <td>9138.40000</td>\n      <td>9138.40000</td>\n      <td>9138.40000</td>\n      <td>9139.155661</td>\n      <td>0.005033</td>\n      <td>1</td>\n      <td>5.00000</td>\n      <td>5.00000</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>9145.172902</td>\n      <td>9144.676061</td>\n      <td>-0.496841</td>\n      <td>-1.012468</td>\n      <td>0.515626</td>\n      <td>9127.00</td>\n      <td>9158.30000</td>\n      <td>63.578275</td>\n      <td>37.912694</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1777675</th>\n      <td>1593561540</td>\n      <td>9137.10000</td>\n      <td>9137.10000</td>\n      <td>9133.90000</td>\n      <td>9136.527830</td>\n      <td>0.933489</td>\n      <td>5</td>\n      <td>-4.50000</td>\n      <td>0.00000</td>\n      <td>4.50000</td>\n      <td>...</td>\n      <td>9144.986573</td>\n      <td>9144.322747</td>\n      <td>-0.663826</td>\n      <td>-1.004805</td>\n      <td>0.340979</td>\n      <td>9127.00</td>\n      <td>9158.30000</td>\n      <td>77.955272</td>\n      <td>37.897886</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>1777042 rows × 23 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_scalar = MinMaxScaler()\n",
    "dataset[[\"rsi\", \"stosc_k\", \"stosc_d\"]] = mm_scalar.fit_transform(dataset[[\"rsi\", \"stosc_k\", \"stosc_d\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "               time        open        high         low        close  \\\n358      1383503820   207.67198   207.67198   207.67198   207.562051   \n359      1383505860   207.43988   207.43988   206.89000   207.287911   \n360      1383507660   207.59295   207.75000   207.59295   207.518955   \n361      1383507720   207.75000   207.75000   207.75000   207.634478   \n362      1383510600   207.75000   207.75000   207.75000   207.692239   \n...             ...         ...         ...         ...          ...   \n1777671  1593561180  9147.00000  9147.00000  9147.00000  9147.145284   \n1777672  1593561240  9147.00000  9147.00000  9145.70000  9146.422642   \n1777673  1593561360  9144.70000  9144.70000  9133.40000  9139.911321   \n1777674  1593561480  9138.40000  9138.40000  9138.40000  9139.155661   \n1777675  1593561540  9137.10000  9137.10000  9133.90000  9136.527830   \n\n            volume  trades  rsi_diff    rsi_u     rsi_d  ...     ema_slow  \\\n358       0.100000       1  -0.00418  0.00000   0.00418  ...   203.646855   \n359       1.263692       5  -0.65821  0.00000   0.65821  ...   203.702506   \n360       0.100000       2   0.73623  0.73623   0.00000  ...   203.769407   \n361       0.100000       1   0.00000  0.00000   0.00000  ...   203.835202   \n362       0.314611       1   0.00000  0.00000   0.00000  ...   203.899909   \n...            ...     ...       ...      ...       ...  ...          ...   \n1777671   0.002000       1   2.00000  2.00000   0.00000  ...  9145.482921   \n1777672   0.187285       5  -1.30000  0.00000   1.30000  ...  9145.486510   \n1777673  23.300000      36 -12.30000  0.00000  12.30000  ...  9145.286733   \n1777674   0.005033       1   5.00000  5.00000   0.00000  ...  9145.172902   \n1777675   0.933489       5  -4.50000  0.00000   4.50000  ...  9144.986573   \n\n            ema_fast      MACD    signal  macd_diff  stosc_high_price  \\\n358       205.570957  1.924102  2.995043  -1.070941            201.04   \n359       205.618262  1.915756  2.971323  -1.055567            201.04   \n360       205.688155  1.918748  2.948189  -1.029441            201.04   \n361       205.755757  1.920555  2.925604  -1.005049            201.04   \n362       205.821142  1.921233  2.903530  -0.982297            201.04   \n...              ...       ...       ...        ...               ...   \n1777671  9145.263964 -0.218958 -1.056774   0.837816           9127.00   \n1777672  9145.278260 -0.208250 -1.038125   0.829875           9127.00   \n1777673  9144.888809 -0.397924 -1.024055   0.626131           9127.00   \n1777674  9144.676061 -0.496841 -1.012468   0.515626           9127.00   \n1777675  9144.322747 -0.663826 -1.004805   0.340979           9127.00   \n\n         stosc_low_price   stosc_k   stosc_d   optimal  \n358            207.96822  0.042758  0.238590 -0.715267  \n359            207.96822  0.137763  0.239033  0.627125  \n360            207.96822  0.031497  0.239011  0.000000  \n361            207.96822  0.031497  0.238988  0.000000  \n362            207.96822  0.031497  0.239008  0.000000  \n...                  ...       ...       ...       ...  \n1777671       9158.30000  0.361022  0.411425  0.000000  \n1777672       9158.30000  0.402556  0.408831  0.000000  \n1777673       9158.30000  0.795527  0.407955  0.000000  \n1777674       9158.30000  0.635783  0.406806  0.000000  \n1777675       9158.30000  0.779553  0.406631  0.000000  \n\n[1777042 rows x 23 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>volume</th>\n      <th>trades</th>\n      <th>rsi_diff</th>\n      <th>rsi_u</th>\n      <th>rsi_d</th>\n      <th>...</th>\n      <th>ema_slow</th>\n      <th>ema_fast</th>\n      <th>MACD</th>\n      <th>signal</th>\n      <th>macd_diff</th>\n      <th>stosc_high_price</th>\n      <th>stosc_low_price</th>\n      <th>stosc_k</th>\n      <th>stosc_d</th>\n      <th>optimal</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>358</th>\n      <td>1383503820</td>\n      <td>207.67198</td>\n      <td>207.67198</td>\n      <td>207.67198</td>\n      <td>207.562051</td>\n      <td>0.100000</td>\n      <td>1</td>\n      <td>-0.00418</td>\n      <td>0.00000</td>\n      <td>0.00418</td>\n      <td>...</td>\n      <td>203.646855</td>\n      <td>205.570957</td>\n      <td>1.924102</td>\n      <td>2.995043</td>\n      <td>-1.070941</td>\n      <td>201.04</td>\n      <td>207.96822</td>\n      <td>0.042758</td>\n      <td>0.238590</td>\n      <td>-0.715267</td>\n    </tr>\n    <tr>\n      <th>359</th>\n      <td>1383505860</td>\n      <td>207.43988</td>\n      <td>207.43988</td>\n      <td>206.89000</td>\n      <td>207.287911</td>\n      <td>1.263692</td>\n      <td>5</td>\n      <td>-0.65821</td>\n      <td>0.00000</td>\n      <td>0.65821</td>\n      <td>...</td>\n      <td>203.702506</td>\n      <td>205.618262</td>\n      <td>1.915756</td>\n      <td>2.971323</td>\n      <td>-1.055567</td>\n      <td>201.04</td>\n      <td>207.96822</td>\n      <td>0.137763</td>\n      <td>0.239033</td>\n      <td>0.627125</td>\n    </tr>\n    <tr>\n      <th>360</th>\n      <td>1383507660</td>\n      <td>207.59295</td>\n      <td>207.75000</td>\n      <td>207.59295</td>\n      <td>207.518955</td>\n      <td>0.100000</td>\n      <td>2</td>\n      <td>0.73623</td>\n      <td>0.73623</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>203.769407</td>\n      <td>205.688155</td>\n      <td>1.918748</td>\n      <td>2.948189</td>\n      <td>-1.029441</td>\n      <td>201.04</td>\n      <td>207.96822</td>\n      <td>0.031497</td>\n      <td>0.239011</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>361</th>\n      <td>1383507720</td>\n      <td>207.75000</td>\n      <td>207.75000</td>\n      <td>207.75000</td>\n      <td>207.634478</td>\n      <td>0.100000</td>\n      <td>1</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>203.835202</td>\n      <td>205.755757</td>\n      <td>1.920555</td>\n      <td>2.925604</td>\n      <td>-1.005049</td>\n      <td>201.04</td>\n      <td>207.96822</td>\n      <td>0.031497</td>\n      <td>0.238988</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>362</th>\n      <td>1383510600</td>\n      <td>207.75000</td>\n      <td>207.75000</td>\n      <td>207.75000</td>\n      <td>207.692239</td>\n      <td>0.314611</td>\n      <td>1</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>203.899909</td>\n      <td>205.821142</td>\n      <td>1.921233</td>\n      <td>2.903530</td>\n      <td>-0.982297</td>\n      <td>201.04</td>\n      <td>207.96822</td>\n      <td>0.031497</td>\n      <td>0.239008</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1777671</th>\n      <td>1593561180</td>\n      <td>9147.00000</td>\n      <td>9147.00000</td>\n      <td>9147.00000</td>\n      <td>9147.145284</td>\n      <td>0.002000</td>\n      <td>1</td>\n      <td>2.00000</td>\n      <td>2.00000</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>9145.482921</td>\n      <td>9145.263964</td>\n      <td>-0.218958</td>\n      <td>-1.056774</td>\n      <td>0.837816</td>\n      <td>9127.00</td>\n      <td>9158.30000</td>\n      <td>0.361022</td>\n      <td>0.411425</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1777672</th>\n      <td>1593561240</td>\n      <td>9147.00000</td>\n      <td>9147.00000</td>\n      <td>9145.70000</td>\n      <td>9146.422642</td>\n      <td>0.187285</td>\n      <td>5</td>\n      <td>-1.30000</td>\n      <td>0.00000</td>\n      <td>1.30000</td>\n      <td>...</td>\n      <td>9145.486510</td>\n      <td>9145.278260</td>\n      <td>-0.208250</td>\n      <td>-1.038125</td>\n      <td>0.829875</td>\n      <td>9127.00</td>\n      <td>9158.30000</td>\n      <td>0.402556</td>\n      <td>0.408831</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1777673</th>\n      <td>1593561360</td>\n      <td>9144.70000</td>\n      <td>9144.70000</td>\n      <td>9133.40000</td>\n      <td>9139.911321</td>\n      <td>23.300000</td>\n      <td>36</td>\n      <td>-12.30000</td>\n      <td>0.00000</td>\n      <td>12.30000</td>\n      <td>...</td>\n      <td>9145.286733</td>\n      <td>9144.888809</td>\n      <td>-0.397924</td>\n      <td>-1.024055</td>\n      <td>0.626131</td>\n      <td>9127.00</td>\n      <td>9158.30000</td>\n      <td>0.795527</td>\n      <td>0.407955</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1777674</th>\n      <td>1593561480</td>\n      <td>9138.40000</td>\n      <td>9138.40000</td>\n      <td>9138.40000</td>\n      <td>9139.155661</td>\n      <td>0.005033</td>\n      <td>1</td>\n      <td>5.00000</td>\n      <td>5.00000</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>9145.172902</td>\n      <td>9144.676061</td>\n      <td>-0.496841</td>\n      <td>-1.012468</td>\n      <td>0.515626</td>\n      <td>9127.00</td>\n      <td>9158.30000</td>\n      <td>0.635783</td>\n      <td>0.406806</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1777675</th>\n      <td>1593561540</td>\n      <td>9137.10000</td>\n      <td>9137.10000</td>\n      <td>9133.90000</td>\n      <td>9136.527830</td>\n      <td>0.933489</td>\n      <td>5</td>\n      <td>-4.50000</td>\n      <td>0.00000</td>\n      <td>4.50000</td>\n      <td>...</td>\n      <td>9144.986573</td>\n      <td>9144.322747</td>\n      <td>-0.663826</td>\n      <td>-1.004805</td>\n      <td>0.340979</td>\n      <td>9127.00</td>\n      <td>9158.30000</td>\n      <td>0.779553</td>\n      <td>0.406631</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>1777042 rows × 23 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = dataset[[\"close\", \"rsi\", \"MACD\", \"stosc_k\", \"stosc_d\", \"optimal\"]]\n",
    "optimal_features = filtered_dataset.drop(\"optimal\", axis=1)\n",
    "is_optimal = filtered_dataset[['optimal']]\n",
    "\n",
    "raw_x_train_df, raw_x_test_df, raw_y_train_df, raw_y_test_df = train_test_split(optimal_features, is_optimal, test_size=0.2, shuffle=False)\n",
    "\n",
    "classify_all = np.array(filtered_dataset)\n",
    "optimal_all = np.array(is_optimal)\n",
    "regress_X_train = np.array(raw_x_train_df)\n",
    "regress_X_test = np.array(raw_x_test_df)\n",
    "regress_y_train = np.array(raw_y_train_df)\n",
    "regress_y_test = np.array(raw_y_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Models</h3>\n",
    "<p>Linear Regression </p>\n",
    "<p>Random Forest</p>\n",
    "<p>Gradeint Boosting</p>\n",
    "<p>Neural Network</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.02745161882708469"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "#linear\n",
    "ridge_model = Ridge(alpha=0.1)\n",
    "ridge_model.fit(regress_X_train, regress_y_train)\n",
    "ridge_model.score(regress_X_test, regress_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "-0.0072682261698842066"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "#random forest\n",
    "random_forest = RandomForestRegressor(n_jobs=-1)\n",
    "random_forest.fit(regress_X_train, regress_y_train)\n",
    "random_forest.score(regress_X_test, regress_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.048338478300421595"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "#gradient boost\n",
    "xgboost = XGBRegressor()\n",
    "xgboost.fit(regress_X_train, regress_y_train)\n",
    "xgboost.score(regress_X_test, regress_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "-1.2318603670101296"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "#random forest with parameters changed\n",
    "clf = RandomForestRegressor(random_state=69420, max_features=None, n_estimators=100, bootstrap=False, n_jobs=-1, min_samples_leaf=1)\n",
    "clf.fit(regress_X_train, regress_y_train)\n",
    "clf.score(regress_X_test, regress_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, RNN, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(regress_X_train, (regress_X_train.shape[0], 1, regress_X_train.shape[1]))\n",
    "y_train = regress_y_train\n",
    "x_test = np.reshape(regress_X_test, (regress_X_test.shape[0], 1, regress_X_test.shape[1]))\n",
    "y_test = regress_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(512, activation='relu', return_sequences=True))\n",
    "BatchNormalization()\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(128, activation='relu'))\n",
    "BatchNormalization()\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=1e-3, decay=1e-5)\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/20\n50773/50773 [==============================] - 737s 15ms/step - loss: 0.1656 - accuracy: 0.6897 - val_loss: 0.1185 - val_accuracy: 0.7295\nEpoch 2/20\n50773/50773 [==============================] - 743s 15ms/step - loss: 0.1592 - accuracy: 0.6900 - val_loss: 0.1185 - val_accuracy: 0.7295\nEpoch 3/20\n50773/50773 [==============================] - 737s 15ms/step - loss: 0.1592 - accuracy: 0.6900 - val_loss: 0.1187 - val_accuracy: 0.7295\nEpoch 4/20\n50773/50773 [==============================] - 737s 15ms/step - loss: 0.1592 - accuracy: 0.6900 - val_loss: 0.1186 - val_accuracy: 0.7295\nEpoch 5/20\n50773/50773 [==============================] - 749s 15ms/step - loss: 0.1592 - accuracy: 0.6900 - val_loss: 0.1185 - val_accuracy: 0.7295\nEpoch 6/20\n50773/50773 [==============================] - 704s 14ms/step - loss: 0.1592 - accuracy: 0.6900 - val_loss: 0.1185 - val_accuracy: 0.7295\nEpoch 7/20\n50773/50773 [==============================] - 651s 13ms/step - loss: 0.1592 - accuracy: 0.6900 - val_loss: 0.1185 - val_accuracy: 0.7295\nEpoch 8/20\n50773/50773 [==============================] - 649s 13ms/step - loss: 0.1592 - accuracy: 0.6900 - val_loss: 0.1186 - val_accuracy: 0.7295\nEpoch 9/20\n50773/50773 [==============================] - 692s 14ms/step - loss: 0.1592 - accuracy: 0.6900 - val_loss: 0.1186 - val_accuracy: 0.7295\nEpoch 10/20\n50773/50773 [==============================] - 717s 14ms/step - loss: 0.1592 - accuracy: 0.6900 - val_loss: 0.1188 - val_accuracy: 0.7295\nEpoch 11/20\n50773/50773 [==============================] - 641s 13ms/step - loss: 0.1592 - accuracy: 0.6900 - val_loss: 0.1188 - val_accuracy: 0.7295\nEpoch 12/20\n50773/50773 [==============================] - 661s 13ms/step - loss: 0.1592 - accuracy: 0.6900 - val_loss: 0.1186 - val_accuracy: 0.7295\nEpoch 13/20\n50773/50773 [==============================] - 7213s 142ms/step - loss: 0.1592 - accuracy: 0.6900 - val_loss: 0.1186 - val_accuracy: 0.7295\nEpoch 14/20\n50773/50773 [==============================] - 1674s 33ms/step - loss: 0.1592 - accuracy: 0.6900 - val_loss: 0.1186 - val_accuracy: 0.7295\nEpoch 15/20\n50773/50773 [==============================] - 673s 13ms/step - loss: 0.1592 - accuracy: 0.6900 - val_loss: 0.1185 - val_accuracy: 0.7295\nEpoch 16/20\n50773/50773 [==============================] - 680s 13ms/step - loss: 0.1592 - accuracy: 0.6900 - val_loss: 0.1185 - val_accuracy: 0.7295\nEpoch 17/20\n50773/50773 [==============================] - 630s 12ms/step - loss: 0.1592 - accuracy: 0.6900 - val_loss: 0.1186 - val_accuracy: 0.7295\nEpoch 18/20\n50773/50773 [==============================] - 634s 12ms/step - loss: 0.1592 - accuracy: 0.6900 - val_loss: 0.1186 - val_accuracy: 0.7295\nEpoch 19/20\n50773/50773 [==============================] - 628s 12ms/step - loss: 0.1592 - accuracy: 0.6900 - val_loss: 0.1187 - val_accuracy: 0.7295\nEpoch 20/20\n50773/50773 [==============================] - 619s 12ms/step - loss: 0.1592 - accuracy: 0.6900 - val_loss: 0.1185 - val_accuracy: 0.7295\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x140f20130>"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=20, batch_size=28, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}